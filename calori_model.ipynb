{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dastanrab/Data-Structures/blob/master/calori_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6IXGv8J_b7J"
      },
      "source": [
        "!pip install unsloth\n",
        "!pip install bitsandbytes\n",
        "!pip install trl\n",
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install protobuf==3.20.3\n",
        "!git clone https://github.com/ggml-org/llama.cpp\n",
        "%cd llama.cpp\n",
        "# !cmake -B build\n",
        "# !cmake --build build --config Release\n",
        "git checkout b3345\n",
        "git submodule update --init --recursive\n",
        "make clean\n",
        "make all -j\n",
        "git log -\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load base model with Unsloth\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = 'unsloth/Phi-3-mini-4k-instruct-bnb-4bit',\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True\n",
        ")\n",
        "\n",
        "# Load dataset directly from Hugging Face\n",
        "dataset = load_dataset(\"Codatta/MM-Food-100K\", split=\"train\")"
      ],
      "id": "k6IXGv8J_b7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd llama.cpp\n",
        "git checkout b3345\n",
        "git submodule update --init --recursive\n",
        "make clean\n",
        "make all -j\n",
        "git log -1"
      ],
      "metadata": {
        "id": "aB_eBgn-wSxm"
      },
      "id": "aB_eBgn-wSxm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWOKaXMZATZg"
      },
      "id": "lWOKaXMZATZg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ipoVnP9_b7M"
      },
      "source": [
        "import json\n",
        "# Map dataset to text format for SFTTrainer\n",
        "def to_text(ex):\n",
        "    # ورودی (پرومپت) از ستون‌های دیتاست ساخته میشه\n",
        "    prompt = (\n",
        "        f\"Dish: {ex['dish_name']}\\n\"\n",
        "        f\"Ingredients: {', '.join(ex['ingredients'])}\\n\"\n",
        "        f\"Portion: {', '.join(ex['portion_size'])}\\n\"\n",
        "        f\"Cooking method: {ex['cooking_method']}\"\n",
        "    )\n",
        "\n",
        "    # خروجی (ریسپانس) پروفایل غذاییه\n",
        "    response = json.dumps(ex[\"nutritional_profile\"], ensure_ascii=False)\n",
        "\n",
        "    msgs = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": response},\n",
        "    ]\n",
        "    return {\n",
        "        \"text\": tokenizer.apply_chat_template(\n",
        "            msgs, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(to_text, remove_columns=dataset.column_names)"
      ],
      "id": "7ipoVnP9_b7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Lnd_Wf_b7M"
      },
      "source": [
        "# Prepare model for LoRA fine-tuning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,\n",
        "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0,\n",
        "    bias = 'none',\n",
        "    use_gradient_checkpointing = 'unsloth'\n",
        ")"
      ],
      "id": "l6Lnd_Wf_b7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NKlw0oS_b7N"
      },
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    dataset_text_field = 'text',\n",
        "    max_seq_length = 2048,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps = 60,  # small for demo, increase for real training\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\",\n",
        "        optim = \"adamw_8bit\",\n",
        "        num_train_epochs = 1\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "7NKlw0oS_b7N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0kGl20Y_b7N"
      },
      "source": [
        "# Test inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Dish: Fried Chicken\\nIngredients: chicken, breading, oil\\nPortion: 300g\\nCooking method: Frying\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(response)"
      ],
      "id": "K0kGl20Y_b7N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL78aoQg_b7O"
      },
      "source": [
        "# Export to GGUF for Ollama\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    \"/content/gguf_food_model\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    maximum_memory_usage = 0.3)"
      ],
      "id": "jL78aoQg_b7O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !python3 convert_hf_to_gguf.py ../gguf_food_model --outfile ../gguf_food_model_final.gguf\n",
        "# %%bash\n",
        "# git clone https://github.com/ggerganov/llama.cpp\n",
        "# cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n",
        "!./llama.cpp/quantize /content/gguf_food_model_final.gguf /content/gguf_food_model q8_0"
      ],
      "metadata": {
        "id": "gPd3XxaXt_cc"
      },
      "id": "gPd3XxaXt_cc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}